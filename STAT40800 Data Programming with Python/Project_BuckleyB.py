# Data Programming with Python Project
#
# Brian Buckley 14203480

# Strand 1 - Statistical Modelling/Machine Learning
#
# Predicting the efficacy of Weight Lifting Exercises from telemetry data
# generated by fitness tracking devices.


## Objective

# The purpose of this exercise is to use the weight-lifting exercise data set of the 
# Human Activity Recognition program from Groupware 
# (http://groupware.les.inf.pucrio.br/har). 

# In these data, 6 participants performed weight-lifting exercises in 5 different techniques, 
# one of which is the correct technique.  Data from a set of accelerometers was collected and 
# a machine learning algorithm used to predict the technique performed from the telemetry  
# data.

# Our objective is to use the "classe" variable (classifying the 5 different techniques) in the 
# test set to predict the Groupware results


## Data Cleansing

from pandas import Series, DataFrame
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Mac path = '/Volumes/BrianB/Courses/UCD Data Analytics/STAT40800 Data Programming with Python/Assessments/Project/'

path = 'F:/Courses/UCD Data Analytics/STAT40800 Data Programming with Python/Assessments/Project/'
df_training = pd.read_csv(path + "har-training.csv")
df_testing = pd.read_csv(path + "har-testing.csv")

np.shape(df_training) # (19622, 160)

# Check for NaNs
df_training.isnull().sum().sum() # 1921600 NaNs present

# When we look at the data using np.shape() we see that it has 19622 observations on 160 variables.
# We also note a lot of NAN and empty columns plus some irrelevant variables like user name, timestamp, etc.
# So first we cleaned it up by removing all NANs

df1 = df_training.fillna(0)
df1.columns.values

# Next we removed identifier columns such as name, timestamps etc

df2 = df1.drop(df1.columns[:7], axis=1)
np.shape(df2) # (19622,153)

# The are also some divide by zero error cells ("#DIV/0!") that we want to set to 0

df3 = df2.replace({'#DIV/0!': 0}, regex=True)

# do the same for the test set
test1 = df_testing.fillna(0)
test2 = test1.drop(test1.columns[:7], axis=1)

## Cross Validation Set

# We allocated 25% of the traing data for cross validation to test the accuracy of our model.

np.random.seed(100)
msk = np.random.rand(len(df3)) < 0.75
train = df3[msk]
crossval = df3[~msk]

## Correlation reduction

# As we still have a lot of variables (153) we used a correlation matrix to identify if 
# there are any highly correlated variables in the data (collinearity).
# Highly correlated variables can be removed thereby improving the efficiency of our process 
# and reducing overfitting.

corr_pearson = train.corr(method='pearson')
# assume paired variable is the last, then remove correlation with itself
corr_pair = corr_pearson.ix[-1][:-1]

# variables sorted from the most predictive
corr_pair.sort(ascending=False)
var = corr_pearson.iloc[:-1,:-1] 

# We used a correlation coefficient r > 0.5 to remove highly correlated variables.
r = 0.5
significant_corrs = (var[abs(var) > r][var != 1.0]).unstack().dropna().to_dict()

uniq_sig_corrs= pd.DataFrame(list(set([(tuple(sorted(key)), significant_corrs[key]) for key in significant_corrs])), columns=['attribute pair', 'correlation'])
# sort by absolute value to account for negative correlations
uniq_sig_corrs= uniq_sig_corrs.ix[abs(uniq_sig_corrs['correlation']).argsort()[::-1]]
len(uniq_sig_corrs)				
# We can see 545 highly correlated pairs

# Look at the top 10
uniq_sig_corrs.head(n=10)


# Visualise using a cross-correlation plot

import seaborn as sns
fig, ax = plt.subplots(figsize=(10, 10))
sns.corrplot(train, ax=ax, annot=False)

## Dimensionality reduction

# The correlation plot above identifies correlation by the strength of the colour.  
# As can be seen in the plot it looks like we do have quite a few correlated variables.

## Now we drop the correlated variables from the data

drop_list = []
for i in uniq_sig_corrs.values:
    drop_list.append(i[0][1])
    
# Drop duplicates
drop_list2 = list(set(drop_list))

train_reduced = train.drop(drop_list2, axis=1)
np.shape(train_reduced) # (14755,63)

# Now we have reduced our variables to 63

# Check correlation for train_reduced
# If we plot the correlation matrix again using train_reduced we confirm the data is 
# not highly correlated now.
fig, ax = plt.subplots(figsize=(10, 10))
sns.corrplot(train_reduced, ax=ax, annot=False)

# drop the same correlated columns from the test set and correlation set
test_reduced = test2.drop(drop_list2, axis=1)
cv_reduced = crossval.drop(drop_list2, axis=1)

# pd.scatter_matrix(train_reduced)

# try sklearn feature reduction
from sklearn import cluster

# 1: feature agglomeration

X = train
X = X.drop('classe',axis=1)
agglo = cluster.FeatureAgglomeration(n_clusters=32)
agglo.fit(X)
train_reduced_sklearn = agglo.transform(X)
train_scaled = scaler.fit_transform(train_reduced_sklearn)

## Data Scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

train_scaled = train_reduced.drop('classe',axis=1)
test_scaled = test_reduced.drop('classe',axis=1)
cv_scaled = cv_reduced.drop('classe',axis=1)
train_scaled = scaler.fit_transform(train_scaled)  # compute mean, std and transform
test_scaled = scaler.fit_transform(test_scaled) 
cv_scaled = scaler.fit_transform(cv_scaled) 

## Model fitting

# We fit a random forest model to predict the 'classe' variable using everything else as a 
# predictor.  We used random forest as that is said to be the most accurate classifier where 
# a large data set is available (we will compare with SVM later).

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix

clf = RandomForestClassifier(n_jobs=2)            # enables parallel processing
y, _ = pd.factorize(train_reduced['classe'])      # factor A-E to 0-4
clf.fit(train_scaled, y)

%timeit clf.fit(train_scaled, y)
#1 loops, best of 3: 238 ms per loop

## Model Accuracy using the Cross Validation Data

# Now we want to test the accuracy of our model so we crossvalidate the model using the 
# remaining 25% of data.
y_cv = clf.predict(cv_scaled)

# The accuracy of our model is tested by comparing the predicted values from our model to 
# the actual values in the CV data.
compare = pd.Categorical(crossval['classe'])
accuracy = float(sum(y_cv == compare.labels))/float(len(cv_scaled))
accuracy  # 0.9407301066447908

# Our model accuracy is 94%.
# We also want to know the out-of-sample error.  This is the complement of accuracy.
oos_error = 1-accuracy
oos_error # 0.05926989335520916
# In our case the out-of-sample error is 6%.

## Test the model against the test set
y_pred = clf.predict(test_scaled)
ct = pd.crosstab(test_reduced['classe'], y_pred, rownames=['actual'], colnames=['predicted'])    

# Misclassification rate for the test set
mr_test = 1 - float(np.trace(ct.values))/float(np.sum(ct.values))
mr_test # 0.05


## Compare with a Support Vector Machine

from sklearn import svm
clf = svm.LinearSVC()
y, _ = pd.factorize(train_reduced['classe'])
clf.fit(train_scaled, y)

%timeit clf.fit(train_scaled, y)
# 1 loops, best of 3: 3.83 s per loop
# The SVM is significantly less performant compared with the Random Forest in this case.

# Cross Validation test
y_cv = clf.predict(cv_scaled)
compare = pd.Categorical(crossval['classe'])
accuracy = float(sum(y_cv == compare.labels))/float(len(cv_scaled))
accuracy  # 0.5133305988515177
oos_error = 1-accuracy
oos_error # 0.48666940114848234

# SVM accuracy = 51% and out-of-sample error = 49%

## Test the model against the test set
y_pred = clf.predict(test_scaled)
ct = pd.crosstab(test_reduced['classe'], y_pred, rownames=['actual'], colnames=['predicted'])

# Misclassification rate for the test set
mr_test = 1 - float(np.trace(ct.values))/float(np.sum(ct.values))
mr_test # 0.4

# The SVM has significantly less predictive accuracy than the Random Forest for this data analysis.
